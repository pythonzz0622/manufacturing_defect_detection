{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user304/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from custom_dataloader import import_data_loader\n",
    "import custom_dataloader\n",
    "from custom_model import retina_fpn_swin_l\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import focal_loss\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################### transformer ##############################\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "transformer = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Flip(),\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=(0, 90), p=0.7),\n",
    "        A.Compose([\n",
    "            A.HorizontalFlip(always_apply=True),\n",
    "            A.Rotate(limit=(-90, 0), p=0.7)\n",
    "        ])\n",
    "    ], p=0.7),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=(\n",
    "            0, 0.2), contrast_limit=(0, 0.3), p=0.7),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.7),\n",
    "        A.Sharpen(alpha=(0.2, 0.8), lightness=(0.5, 1.0), p=0.7)\n",
    "    ], p=0.7),\n",
    "    A.Normalize(mean=mean, std=std,\n",
    "                max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_ids']))\n",
    "\n",
    "val_transformer = A.Compose([\n",
    "    A.Resize(height=512, width=512),\n",
    "    A.Normalize(mean=mean, std=std,\n",
    "                max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "],\n",
    "    bbox_params=A.BboxParams(format='coco', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "##########################################################################################\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_dataset = custom_dataloader.CustomDataset(\n",
    "    '../info/train.json', transformer=transformer)\n",
    "train_dataloader = import_data_loader(\n",
    "    '../info/train.json', transformer=transformer)\n",
    "val_dataset = custom_dataloader.TestCustomDataset(\n",
    "    '../info/test.json', transformer=val_transformer)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=8,\n",
    "                            collate_fn=custom_dataloader.collate_fn)\n",
    "\n",
    "\n",
    "de_std = tuple(std * 255 for std in std)\n",
    "de_mean = tuple(mean * 255 for mean in mean)\n",
    "\n",
    "\n",
    "model = retina_fpn_swin_l()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                        lr=0.0001 / 8,\n",
    "                        betas=(0.9, 0.999),\n",
    "                        weight_decay=0.05,\n",
    "                        )\n",
    "\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load('./test3.pth'))\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "cls_losses = 0\n",
    "bbox_losses = 0\n",
    "total_losses = 0\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     for i, (img_metas, images, bboxes, labels) in enumerate(train_dataloader):\n",
    "#         images = images.cuda()\n",
    "#         model.train()\n",
    "\n",
    "#         bboxes = [bbox.cuda() for bbox in bboxes]\n",
    "#         labels = [label.cuda() for label in labels]\n",
    "#         outputs = model(images)\n",
    "#         losses = model.bbox_head.loss(\n",
    "#             cls_scores=outputs[0],\n",
    "#             bbox_preds=outputs[1],\n",
    "#             gt_bboxes=bboxes,\n",
    "#             gt_labels=labels,\n",
    "#             img_metas=img_metas\n",
    "#         )\n",
    "\n",
    "#         loss_cls_total = losses['loss_cls'][0] + \\\n",
    "#             losses['loss_cls'][1] + losses['loss_cls'][2]\n",
    "#         loss_bbox_total = losses['loss_bbox'][0] + \\\n",
    "#             losses['loss_bbox'][1] + losses['loss_bbox'][2]\n",
    "#         total_loss = loss_cls_total + loss_bbox_total\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         cls_losses += loss_cls_total\n",
    "#         bbox_losses += loss_bbox_total\n",
    "#         total_losses += total_loss\n",
    "#         if i % 100 == 0 and i > 0:\n",
    "#             print(f'[{i} / {len(train_dataloader)}]')\n",
    "#             print(\n",
    "#                 f'step : {i}\\nloss_bbox : {bbox_losses / 100}  loss_cls_total : {cls_losses /100}  total_loss : {total_losses / 100}')\n",
    "#             cls_losses = 0\n",
    "#             bbox_losses = 0\n",
    "#             total_losses = 0\n",
    "    # if epoch % 10 == 0:\n",
    "    #     cls_losses = 0\n",
    "    #     bbox_losses = 0\n",
    "    #     total_losses = 0\n",
    "    #     with torch.no_grad():\n",
    "    #         model.eval()\n",
    "    #         for j, (img_metas, images, bboxes, labels) in enumerate(val_dataloader):\n",
    "    #             bboxes = [bbox.cuda() for bbox in bboxes]\n",
    "    #             labels = [label.cuda() for label in labels]\n",
    "    #             outputs = model(images)\n",
    "    #             losses = model.bbox_head.loss(\n",
    "    #                 cls_scores=outputs[0],\n",
    "    #                 bbox_preds=outputs[1],\n",
    "    #                 gt_bboxes=bboxes,\n",
    "    #                 gt_labels=labels,\n",
    "    #                 img_metas=img_metas\n",
    "    #             )\n",
    "    #             loss_cls_total = losses['loss_cls'][0] + \\\n",
    "    #                 losses['loss_cls'][1] + losses['loss_cls'][2]\n",
    "    #             loss_bbox_total = losses['loss_bbox'][0] + \\\n",
    "    #                 losses['loss_bbox'][1] + losses['loss_bbox'][2]\n",
    "    #             total_loss = loss_cls_total + loss_bbox_total\n",
    "    #             cls_losses += loss_cls_total\n",
    "    #             bbox_losses += loss_bbox_total\n",
    "    #             total_losses += total_loss\n",
    "\n",
    "    #             bbox_preds = model.bbox_head.get_bboxes(\n",
    "    #                 cls_scores=outputs[0],\n",
    "    #                 bbox_preds=outputs[1],\n",
    "    #                 img_metas=img_metas\n",
    "    #             )\n",
    "    #         print(f'step : {i}\\nloss_bbox : {bbox_losses / len(val_dataloader)}   \\\n",
    "    #                 loss_cls_total : {cls_losses /len(val_dataloader)}  total_loss : {total_losses / len(val_dataloader)}')\n",
    "    # print(f'epoch : {epoch}')\n",
    "    # print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from pycocotools.coco import COCO\n",
    "coco_gt = COCO('../info/test.json')\n",
    "img_name_to_id = {os.path.basename(label_info['file_name']).split('.jpg')[0]: label_info['id']\n",
    "                               for label_info in coco_gt.loadImgs(coco_gt.getImgIds())}\n",
    "\n",
    "val_dataset = custom_dataloader.TestCustomDataset(\n",
    "    '../info/test.json', transformer=val_transformer)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=8,\n",
    "                            collate_fn=custom_dataloader.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find a valid cuDNN algorithm to run convolution",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m bboxes \u001b[39m=\u001b[39m [bbox\u001b[39m.\u001b[39mcuda() \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m bboxes]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m labels \u001b[39m=\u001b[39m [label\u001b[39m.\u001b[39mcuda() \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbbox_head\u001b[39m.\u001b[39mloss(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     cls_scores\u001b[39m=\u001b[39moutputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     bbox_preds\u001b[39m=\u001b[39moutputs[\u001b[39m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     img_metas\u001b[39m=\u001b[39mimg_metas\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss_cls_total \u001b[39m=\u001b[39m losses[\u001b[39m'\u001b[39m\u001b[39mloss_cls\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     losses[\u001b[39m'\u001b[39m\u001b[39mloss_cls\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m losses[\u001b[39m'\u001b[39m\u001b[39mloss_cls\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/users/jiwon/graduation_paper/custom_trainer/custom_model.py:116\u001b[0m, in \u001b[0;36mretina_fpn_swin_l.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 116\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m    117\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(out)\n\u001b[1;32m    118\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_head(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/backbones/swin.py:745\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 745\u001b[0m     x, hw_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embed(x)\n\u001b[1;32m    747\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_abs_pos_embed:\n\u001b[1;32m    748\u001b[0m         h, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mabsolute_pos_embed\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/utils/transformer.py:252\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madap_padding:\n\u001b[1;32m    250\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madap_padding(x)\n\u001b[0;32m--> 252\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection(x)\n\u001b[1;32m    253\u001b[0m out_size \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m])\n\u001b[1;32m    254\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to find a valid cuDNN algorithm to run convolution"
     ]
    }
   ],
   "source": [
    "cls_losses = 0\n",
    "bbox_losses = 0\n",
    "total_losses = 0\n",
    "\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for j, (img_metas, images, bboxes, labels) in enumerate(val_dataloader):\n",
    "        images = images.cuda()\n",
    "        bboxes = [bbox.cuda() for bbox in bboxes]\n",
    "        labels = [label.cuda() for label in labels]\n",
    "        outputs = model(images)\n",
    "        losses = model.bbox_head.loss(\n",
    "            cls_scores=outputs[0],\n",
    "            bbox_preds=outputs[1],\n",
    "            gt_bboxes=bboxes,\n",
    "            gt_labels=labels,\n",
    "            img_metas=img_metas\n",
    "        )\n",
    "        loss_cls_total = losses['loss_cls'][0] + \\\n",
    "            losses['loss_cls'][1] + losses['loss_cls'][2]\n",
    "        loss_bbox_total = losses['loss_bbox'][0] + \\\n",
    "            losses['loss_bbox'][1] + losses['loss_bbox'][2]\n",
    "        total_loss = loss_cls_total + loss_bbox_total\n",
    "        cls_losses += loss_cls_total\n",
    "        bbox_losses += loss_bbox_total\n",
    "        total_losses += total_loss\n",
    "\n",
    "        preds = model.bbox_head.get_bboxes(\n",
    "            cls_scores=outputs[0],\n",
    "            bbox_preds=outputs[1],\n",
    "            img_metas=img_metas ,\n",
    "            rescale = True\n",
    "        )\n",
    "        preds = [(pred[0].cpu().numpy(),pred[1].cpu().numpy())for pred in preds]\n",
    "        preds_list.extend(zip( [img_name_to_id[os.path.basename(img_meta['filename']).split('.')[0]] for img_meta in img_metas], preds))\n",
    "        # break\n",
    "    print(f'step : {j}\\nloss_bbox : {bbox_losses / len(val_dataloader)}   \\\n",
    "            loss_cls_total : {cls_losses /len(val_dataloader)}  total_loss : {total_losses / len(val_dataloader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [{'image_id' : pred_list[0],\n",
    "    'bbox' : utils.type_converter.cv2_to_coco(pred_list[1][0][i][:-1].tolist() , dim =1),\n",
    "    'score' : pred_list[1][0][i][-1],\n",
    "    'category_id' : pred_list[1][1][i]\n",
    "    }\n",
    "    for pred_list in preds_list \n",
    "    for i in range(len(pred_list[1][1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result.json', \"w\") as json_file:\n",
    "    json.dump(result, json_file, indent=4 , cls = utils.type_converter.NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_pred = coco_gt.loadRes('result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 28485,\n",
       " 'bbox': [496.8477478027344, 0.0, 15.152252197265625, 16.7343807220459],\n",
       " 'score': 0.6105105876922607,\n",
       " 'category_id': 2,\n",
       " 'segmentation': [[496.8477478027344,\n",
       "   0.0,\n",
       "   496.8477478027344,\n",
       "   16.7343807220459,\n",
       "   512.0,\n",
       "   16.7343807220459,\n",
       "   512.0,\n",
       "   0.0]],\n",
       " 'area': 253.56355706549948,\n",
       " 'id': 1685,\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_pred.loadAnns(coco_pred.getAnnIds(imgIds = coco_pred.getImgIds()))[-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_gt.loadAnns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycocotools.coco.COCO at 0x7fb5543b5fd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_pred.loadAnns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycocotools.coco.COCO at 0x7fb6185f8910>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_pred.getAnnIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'COCO' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstatisticsserver/home/user304/users/jiwon/graduation_paper/custom_trainer/test_module.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m coco_pred[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'COCO' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "coco_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n"
     ]
    }
   ],
   "source": [
    "cocoEval = COCOeval(coco_gt, coco_pred, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
     ]
    }
   ],
   "source": [
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c3d72f6befbece6777f46293cf4aefc9a1488481f7bfe7e1d49c30006c7994c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
